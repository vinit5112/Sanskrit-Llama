{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8448849,"sourceType":"datasetVersion","datasetId":5034701}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:19.381044Z","iopub.execute_input":"2024-05-18T16:37:19.381687Z","iopub.status.idle":"2024-05-18T16:37:19.385894Z","shell.execute_reply.started":"2024-05-18T16:37:19.381656Z","shell.execute_reply":"2024-05-18T16:37:19.384744Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"df = pd.read_excel(\"/kaggle/input/sanskrit-llama/Sanskrit.xlsx\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:19.387638Z","iopub.execute_input":"2024-05-18T16:37:19.388006Z","iopub.status.idle":"2024-05-18T16:37:21.530007Z","shell.execute_reply.started":"2024-05-18T16:37:19.387982Z","shell.execute_reply":"2024-05-18T16:37:21.529180Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:21.531156Z","iopub.execute_input":"2024-05-18T16:37:21.531473Z","iopub.status.idle":"2024-05-18T16:37:21.542794Z","shell.execute_reply.started":"2024-05-18T16:37:21.531446Z","shell.execute_reply":"2024-05-18T16:37:21.541646Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                             instruction  \\\n0      ये त्रि॑ष॒प्ताः प॑रि॒यन्ति॒ विश्वा॑ रू॒पाणि॒ ब...   \n1      पुन॒रेहि॑ वाचस्पते दे॒वेन॒ मन॑सा स॒ह । वसो॑ष्प...   \n2      इ॒हैवाभि वि त॑नू॒भे आर्त्नी॑ इव॒ ज्यया॑ । वा॒च...   \n3      उप॑हूतो वा॒चस्पति॒रुपा॒स्मान्वा॒चस्पति॑र्ह्वयत...   \n4      वि॒द्मा श॒रस्य॑ पि॒तरं॑ प॒र्जन्यं॒ भूरि॑धायसम्...   \n...                                                  ...   \n20222  अ॒न्यदे॒वाहुर्वि॒द्याया॑ऽअ॒न्यदा॑हु॒रवि॑द्याया...   \n20223  वि॒द्यां चावि॑द्यां च॒ यस्तद्वेदो॒भय॑ꣳ स॒ह।अवि...   \n20224  वा॒युरनि॑लम॒मृत॒मथे॒दं भस्मा॑न्त॒ꣳ शरी॑रम्।ओ३म...   \n20225  अग्ने॒ नय॑ सु॒पथा॑ रा॒येऽअ॒स्मान् विश्वा॑नि दे...   \n20226  हि॒र॒ण्मये॑न॒ पात्रे॑ण स॒त्यस्यापि॑हितं॒ मुखम्...   \n\n                                                response  \n0      The three qualities of Rajogun, Tamogun and Sa...  \n1      O Swami of speech Brahma Dev! Come to me with ...  \n2      O Swami of speech Brahman! Just as by offering...  \n3      We invoke Brahma, the swami of speech. May the...  \n4      The root consciousness is the father of the cl...  \n...                                                  ...  \n20222  We have learned from patient men in particular...  \n20223  Understand both learning and avidya together. ...  \n20224  This body is made of air, nectar etc. The body...  \n20225  O agni! You take us on the best path. You give...  \n20226  The mouth of truth is covered with a golden ve...  \n\n[20227 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ये त्रि॑ष॒प्ताः प॑रि॒यन्ति॒ विश्वा॑ रू॒पाणि॒ ब...</td>\n      <td>The three qualities of Rajogun, Tamogun and Sa...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>पुन॒रेहि॑ वाचस्पते दे॒वेन॒ मन॑सा स॒ह । वसो॑ष्प...</td>\n      <td>O Swami of speech Brahma Dev! Come to me with ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>इ॒हैवाभि वि त॑नू॒भे आर्त्नी॑ इव॒ ज्यया॑ । वा॒च...</td>\n      <td>O Swami of speech Brahman! Just as by offering...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>उप॑हूतो वा॒चस्पति॒रुपा॒स्मान्वा॒चस्पति॑र्ह्वयत...</td>\n      <td>We invoke Brahma, the swami of speech. May the...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>वि॒द्मा श॒रस्य॑ पि॒तरं॑ प॒र्जन्यं॒ भूरि॑धायसम्...</td>\n      <td>The root consciousness is the father of the cl...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20222</th>\n      <td>अ॒न्यदे॒वाहुर्वि॒द्याया॑ऽअ॒न्यदा॑हु॒रवि॑द्याया...</td>\n      <td>We have learned from patient men in particular...</td>\n    </tr>\n    <tr>\n      <th>20223</th>\n      <td>वि॒द्यां चावि॑द्यां च॒ यस्तद्वेदो॒भय॑ꣳ स॒ह।अवि...</td>\n      <td>Understand both learning and avidya together. ...</td>\n    </tr>\n    <tr>\n      <th>20224</th>\n      <td>वा॒युरनि॑लम॒मृत॒मथे॒दं भस्मा॑न्त॒ꣳ शरी॑रम्।ओ३म...</td>\n      <td>This body is made of air, nectar etc. The body...</td>\n    </tr>\n    <tr>\n      <th>20225</th>\n      <td>अग्ने॒ नय॑ सु॒पथा॑ रा॒येऽअ॒स्मान् विश्वा॑नि दे...</td>\n      <td>O agni! You take us on the best path. You give...</td>\n    </tr>\n    <tr>\n      <th>20226</th>\n      <td>हि॒र॒ण्मये॑न॒ पात्रे॑ण स॒त्यस्यापि॑हितं॒ मुखम्...</td>\n      <td>The mouth of truth is covered with a golden ve...</td>\n    </tr>\n  </tbody>\n</table>\n<p>20227 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.rename(columns = {'instruction':'input'}, inplace = True)\ndf.rename(columns = {'response':'output'}, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:21.545341Z","iopub.execute_input":"2024-05-18T16:37:21.545743Z","iopub.status.idle":"2024-05-18T16:37:21.551121Z","shell.execute_reply.started":"2024-05-18T16:37:21.545718Z","shell.execute_reply":"2024-05-18T16:37:21.550362Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df['instruction'] = \"Convert Sanskrit Text to English\"","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:21.552183Z","iopub.execute_input":"2024-05-18T16:37:21.552449Z","iopub.status.idle":"2024-05-18T16:37:21.559360Z","shell.execute_reply.started":"2024-05-18T16:37:21.552427Z","shell.execute_reply":"2024-05-18T16:37:21.558494Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:21.560623Z","iopub.execute_input":"2024-05-18T16:37:21.562426Z","iopub.status.idle":"2024-05-18T16:37:21.573114Z","shell.execute_reply.started":"2024-05-18T16:37:21.562402Z","shell.execute_reply":"2024-05-18T16:37:21.572161Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                                   input  \\\n0      ये त्रि॑ष॒प्ताः प॑रि॒यन्ति॒ विश्वा॑ रू॒पाणि॒ ब...   \n1      पुन॒रेहि॑ वाचस्पते दे॒वेन॒ मन॑सा स॒ह । वसो॑ष्प...   \n2      इ॒हैवाभि वि त॑नू॒भे आर्त्नी॑ इव॒ ज्यया॑ । वा॒च...   \n3      उप॑हूतो वा॒चस्पति॒रुपा॒स्मान्वा॒चस्पति॑र्ह्वयत...   \n4      वि॒द्मा श॒रस्य॑ पि॒तरं॑ प॒र्जन्यं॒ भूरि॑धायसम्...   \n...                                                  ...   \n20222  अ॒न्यदे॒वाहुर्वि॒द्याया॑ऽअ॒न्यदा॑हु॒रवि॑द्याया...   \n20223  वि॒द्यां चावि॑द्यां च॒ यस्तद्वेदो॒भय॑ꣳ स॒ह।अवि...   \n20224  वा॒युरनि॑लम॒मृत॒मथे॒दं भस्मा॑न्त॒ꣳ शरी॑रम्।ओ३म...   \n20225  अग्ने॒ नय॑ सु॒पथा॑ रा॒येऽअ॒स्मान् विश्वा॑नि दे...   \n20226  हि॒र॒ण्मये॑न॒ पात्रे॑ण स॒त्यस्यापि॑हितं॒ मुखम्...   \n\n                                                  output  \\\n0      The three qualities of Rajogun, Tamogun and Sa...   \n1      O Swami of speech Brahma Dev! Come to me with ...   \n2      O Swami of speech Brahman! Just as by offering...   \n3      We invoke Brahma, the swami of speech. May the...   \n4      The root consciousness is the father of the cl...   \n...                                                  ...   \n20222  We have learned from patient men in particular...   \n20223  Understand both learning and avidya together. ...   \n20224  This body is made of air, nectar etc. The body...   \n20225  O agni! You take us on the best path. You give...   \n20226  The mouth of truth is covered with a golden ve...   \n\n                            instruction  \n0      Convert Sanskrit Text to English  \n1      Convert Sanskrit Text to English  \n2      Convert Sanskrit Text to English  \n3      Convert Sanskrit Text to English  \n4      Convert Sanskrit Text to English  \n...                                 ...  \n20222  Convert Sanskrit Text to English  \n20223  Convert Sanskrit Text to English  \n20224  Convert Sanskrit Text to English  \n20225  Convert Sanskrit Text to English  \n20226  Convert Sanskrit Text to English  \n\n[20227 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>output</th>\n      <th>instruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ये त्रि॑ष॒प्ताः प॑रि॒यन्ति॒ विश्वा॑ रू॒पाणि॒ ब...</td>\n      <td>The three qualities of Rajogun, Tamogun and Sa...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>पुन॒रेहि॑ वाचस्पते दे॒वेन॒ मन॑सा स॒ह । वसो॑ष्प...</td>\n      <td>O Swami of speech Brahma Dev! Come to me with ...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>इ॒हैवाभि वि त॑नू॒भे आर्त्नी॑ इव॒ ज्यया॑ । वा॒च...</td>\n      <td>O Swami of speech Brahman! Just as by offering...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>उप॑हूतो वा॒चस्पति॒रुपा॒स्मान्वा॒चस्पति॑र्ह्वयत...</td>\n      <td>We invoke Brahma, the swami of speech. May the...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>वि॒द्मा श॒रस्य॑ पि॒तरं॑ प॒र्जन्यं॒ भूरि॑धायसम्...</td>\n      <td>The root consciousness is the father of the cl...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20222</th>\n      <td>अ॒न्यदे॒वाहुर्वि॒द्याया॑ऽअ॒न्यदा॑हु॒रवि॑द्याया...</td>\n      <td>We have learned from patient men in particular...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>20223</th>\n      <td>वि॒द्यां चावि॑द्यां च॒ यस्तद्वेदो॒भय॑ꣳ स॒ह।अवि...</td>\n      <td>Understand both learning and avidya together. ...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>20224</th>\n      <td>वा॒युरनि॑लम॒मृत॒मथे॒दं भस्मा॑न्त॒ꣳ शरी॑रम्।ओ३म...</td>\n      <td>This body is made of air, nectar etc. The body...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>20225</th>\n      <td>अग्ने॒ नय॑ सु॒पथा॑ रा॒येऽअ॒स्मान् विश्वा॑नि दे...</td>\n      <td>O agni! You take us on the best path. You give...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n    <tr>\n      <th>20226</th>\n      <td>हि॒र॒ण्मये॑न॒ पात्रे॑ण स॒त्यस्यापि॑हितं॒ मुखम्...</td>\n      <td>The mouth of truth is covered with a golden ve...</td>\n      <td>Convert Sanskrit Text to English</td>\n    </tr>\n  </tbody>\n</table>\n<p>20227 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.to_csv(\"data.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:21.574242Z","iopub.execute_input":"2024-05-18T16:37:21.574565Z","iopub.status.idle":"2024-05-18T16:37:21.900967Z","shell.execute_reply.started":"2024-05-18T16:37:21.574544Z","shell.execute_reply":"2024-05-18T16:37:21.900264Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:21.901931Z","iopub.execute_input":"2024-05-18T16:37:21.902201Z","iopub.status.idle":"2024-05-18T16:37:34.079312Z","shell.execute_reply.started":"2024-05-18T16:37:21.902179Z","shell.execute_reply":"2024-05-18T16:37:34.078212Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.23.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import interpreter_login\ninterpreter_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:42:18.870430Z","iopub.execute_input":"2024-05-18T16:42:18.870812Z","iopub.status.idle":"2024-05-18T16:42:23.605766Z","shell.execute_reply.started":"2024-05-18T16:42:18.870782Z","shell.execute_reply":"2024-05-18T16:42:23.604830Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n    Setting a new token will erase the existing one.\n    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  ·····································\nAdd token as git credential? (Y/n)  \n"},{"name":"stdout","text":"Token is valid (permission: write).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:42:28.552034Z","iopub.execute_input":"2024-05-18T16:42:28.552424Z","iopub.status.idle":"2024-05-18T16:42:28.557128Z","shell.execute_reply.started":"2024-05-18T16:42:28.552394Z","shell.execute_reply":"2024-05-18T16:42:28.556125Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"dataset = Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:42:29.537015Z","iopub.execute_input":"2024-05-18T16:42:29.537796Z","iopub.status.idle":"2024-05-18T16:42:29.621097Z","shell.execute_reply.started":"2024-05-18T16:42:29.537765Z","shell.execute_reply":"2024-05-18T16:42:29.620351Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:42:31.947832Z","iopub.execute_input":"2024-05-18T16:42:31.948471Z","iopub.status.idle":"2024-05-18T16:42:31.954248Z","shell.execute_reply.started":"2024-05-18T16:42:31.948441Z","shell.execute_reply":"2024-05-18T16:42:31.953359Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input', 'output', 'instruction'],\n    num_rows: 20227\n})"},"metadata":{}}]},{"cell_type":"code","source":"# dataset.push_to_hub(\"VinitT/Sanskrit-Llama\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:37:34.947444Z","iopub.execute_input":"2024-05-18T16:37:34.948030Z","iopub.status.idle":"2024-05-18T16:37:36.730151Z","shell.execute_reply.started":"2024-05-18T16:37:34.947999Z","shell.execute_reply":"2024-05-18T16:37:36.728881Z"},"trusted":true},"execution_count":27,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVinitT/Sanskrit-Llama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:5391\u001b[0m, in \u001b[0;36mDataset.push_to_hub\u001b[0;34m(self, repo_id, config_name, set_default, split, data_dir, commit_message, commit_description, private, token, revision, branch, create_pr, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5387\u001b[0m     revision \u001b[38;5;241m=\u001b[39m branch\n\u001b[1;32m   5389\u001b[0m api \u001b[38;5;241m=\u001b[39m HfApi(endpoint\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mHF_ENDPOINT, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m-> 5391\u001b[0m repo_url \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   5397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5398\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m repo_url\u001b[38;5;241m.\u001b[39mrepo_id\n\u001b[1;32m   5400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3256\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3253\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   3255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3256\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3257\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n\u001b[1;32m   3259\u001b[0m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:371\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n","\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6648d94f-5187494b7dee802b4c3cf695;8b7abb9f-4d94-423d-835d-ee6811ac1fd5)\n\nInvalid username or password."],"ename":"HfHubHTTPError","evalue":"401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-6648d94f-5187494b7dee802b4c3cf695;8b7abb9f-4d94-423d-835d-ee6811ac1fd5)\n\nInvalid username or password.","output_type":"error"}]},{"cell_type":"code","source":"!git clone https://github.com/OpenAccess-AI-Collective/axolotl\n%cd axolotl\n\n!pip3 install packaging ninja\n!pip3 install -e '.[flash-attn,deepspeed]'","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:43:05.729591Z","iopub.execute_input":"2024-05-18T16:43:05.729960Z","iopub.status.idle":"2024-05-18T16:45:31.543767Z","shell.execute_reply.started":"2024-05-18T16:43:05.729931Z","shell.execute_reply":"2024-05-18T16:45:31.542220Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Cloning into 'axolotl'...\nremote: Enumerating objects: 13155, done.\u001b[K\nremote: Counting objects: 100% (3560/3560), done.\u001b[K\nremote: Compressing objects: 100% (746/746), done.\u001b[K\nremote: Total 13155 (delta 3190), reused 2928 (delta 2761), pack-reused 9595\u001b[K\nReceiving objects: 100% (13155/13155), 5.12 MiB | 24.49 MiB/s, done.\nResolving deltas: 100% (8522/8522), done.\n/kaggle/working/axolotl\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (21.3)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (1.11.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging) (3.1.1)\nObtaining file:///kaggle/working/axolotl\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe (from axolotl==0.4.0)\n  Cloning https://github.com/lm-sys/FastChat.git (to revision 27a05b04a35510afb1d767ae7e5990cbd278f8fe) to /tmp/pip-install-p_u8c6d_/fschat_49ee33f504c0420d8c131cb53fb60bdb\n  Running command git clone --filter=blob:none --quiet https://github.com/lm-sys/FastChat.git /tmp/pip-install-p_u8c6d_/fschat_49ee33f504c0420d8c131cb53fb60bdb\n  Running command git rev-parse -q --verify 'sha^27a05b04a35510afb1d767ae7e5990cbd278f8fe'\n  Running command git fetch -q https://github.com/lm-sys/FastChat.git 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n  Running command git checkout -q 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n  Resolved https://github.com/lm-sys/FastChat.git to commit 27a05b04a35510afb1d767ae7e5990cbd278f8fe\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting packaging==23.2 (from axolotl==0.4.0)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting peft==0.10.0 (from axolotl==0.4.0)\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nCollecting transformers==4.40.2 (from axolotl==0.4.0)\n  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting tokenizers==0.19.1 (from axolotl==0.4.0)\n  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting bitsandbytes==0.43.1 (from axolotl==0.4.0)\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nCollecting accelerate==0.30.1 (from axolotl==0.4.0)\n  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\nCollecting pydantic==2.6.3 (from axolotl==0.4.0)\n  Downloading pydantic-2.6.3-py3-none-any.whl.metadata (84 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting addict (from axolotl==0.4.0)\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nCollecting fire (from axolotl==0.4.0)\n  Downloading fire-0.6.0.tar.gz (88 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: PyYAML>=6.0 in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (2.31.0)\nCollecting datasets==2.19.1 (from axolotl==0.4.0)\n  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (0.2.0)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (0.16.6)\nCollecting einops (from axolotl==0.4.0)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting xformers==0.0.23.post1 (from axolotl==0.4.0)\n  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting optimum==1.16.2 (from axolotl==0.4.0)\n  Downloading optimum-1.16.2-py3-none-any.whl.metadata (17 kB)\nCollecting hf_transfer (from axolotl==0.4.0)\n  Downloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (0.4.6)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (0.58.1)\nRequirement already satisfied: numpy>=1.24.4 in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (1.26.4)\nCollecting evaluate==0.4.1 (from axolotl==0.4.0)\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (1.11.4)\nRequirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (1.2.2)\nRequirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (11.4.1)\nCollecting art (from axolotl==0.4.0)\n  Downloading art-6.2-py3-none-any.whl.metadata (69 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting gradio==3.50.2 (from axolotl==0.4.0)\n  Downloading gradio-3.50.2-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (2.15.1)\nRequirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (2024.2.0)\nRequirement already satisfied: gcsfs in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (2024.2.0)\nCollecting trl==0.8.5 (from axolotl==0.4.0)\n  Downloading trl-0.8.5-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: zstandard==0.22.0 in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (0.22.0)\nRequirement already satisfied: fastcore in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (1.5.29)\nRequirement already satisfied: torch==2.1.2 in /opt/conda/lib/python3.10/site-packages (from axolotl==0.4.0) (2.1.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1->axolotl==0.4.0) (5.9.3)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1->axolotl==0.4.0) (0.23.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.30.1->axolotl==0.4.0) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (2.1.4)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets==2.19.1->axolotl==0.4.0) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.19.1->axolotl==0.4.0) (3.9.1)\nCollecting responses<0.19 (from evaluate==0.4.1->axolotl==0.4.0)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (22.1.0)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (5.3.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (0.108.0)\nCollecting ffmpy (from gradio==3.50.2->axolotl==0.4.0)\n  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client==0.6.1 (from gradio==3.50.2->axolotl==0.4.0)\n  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (0.27.0)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (6.1.1)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (3.7.5)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (3.9.10)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (9.5.0)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (0.25.1)\nCollecting python-multipart (from gradio==3.50.2->axolotl==0.4.0)\n  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\nCollecting semantic-version~=2.0 (from gradio==3.50.2->axolotl==0.4.0)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (4.9.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==3.50.2->axolotl==0.4.0) (0.25.0)\nCollecting websockets<12.0,>=10.0 (from gradio==3.50.2->axolotl==0.4.0)\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting coloredlogs (from optimum==1.16.2->axolotl==0.4.0)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.16.2->axolotl==0.4.0) (1.12)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic==2.6.3->axolotl==0.4.0) (0.6.0)\nCollecting pydantic-core==2.16.3 (from pydantic==2.6.3->axolotl==0.4.0)\n  Downloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->axolotl==0.4.0) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->axolotl==0.4.0) (3.2.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->axolotl==0.4.0) (3.2.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.40.2->axolotl==0.4.0) (2023.12.25)\nCollecting tyro>=0.5.11 (from trl==0.8.5->axolotl==0.4.0)\n  Downloading tyro-0.8.4-py3-none-any.whl.metadata (7.9 kB)\nCollecting deepspeed==0.14.2 (from axolotl==0.4.0)\n  Downloading deepspeed-0.14.2.tar.gz (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting deepspeed-kernels (from axolotl==0.4.0)\n  Downloading deepspeed_kernels-0.0.1.dev1698255861-py3-none-manylinux1_x86_64.whl.metadata (680 bytes)\nCollecting flash-attn==2.5.8 (from axolotl==0.4.0)\n  Downloading flash_attn-2.5.8.tar.gz (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting hjson (from deepspeed==0.14.2->axolotl==0.4.0)\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.2->axolotl==0.4.0) (1.11.1.1)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.14.2->axolotl==0.4.0) (9.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->axolotl==0.4.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->axolotl==0.4.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->axolotl==0.4.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->axolotl==0.4.0) (2024.2.2)\nCollecting cmake>=3.24 (from deepspeed-kernels->axolotl==0.4.0)\n  Downloading cmake-3.29.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastcore->axolotl==0.4.0) (23.3.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->axolotl==0.4.0) (1.16.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->axolotl==0.4.0) (2.4.0)\nCollecting markdown2[all] (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0)\n  Downloading markdown2-2.4.13-py2.py3-none-any.whl.metadata (2.0 kB)\nCollecting nh3 (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0)\n  Downloading nh3-0.2.17-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0) (3.0.42)\nRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.10/site-packages (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0) (13.7.0)\nCollecting shortuuid (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0)\n  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\nCollecting tiktoken (from fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0)\n  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: decorator>4.1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs->axolotl==0.4.0) (5.1.1)\nRequirement already satisfied: google-auth>=1.2 in /opt/conda/lib/python3.10/site-packages (from gcsfs->axolotl==0.4.0) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib in /opt/conda/lib/python3.10/site-packages (from gcsfs->axolotl==0.4.0) (1.2.0)\nRequirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (from gcsfs->axolotl==0.4.0) (1.44.0)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->axolotl==0.4.0) (0.41.1)\nRequirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.10/site-packages (from s3fs->axolotl==0.4.0) (2.12.3)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->axolotl==0.4.0) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->axolotl==0.4.0) (1.51.1)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard->axolotl==0.4.0) (3.5.2)\nRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->axolotl==0.4.0) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->axolotl==0.4.0) (69.0.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->axolotl==0.4.0) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->axolotl==0.4.0) (3.0.2)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->axolotl==0.4.0) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->axolotl==0.4.0) (3.1.41)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->axolotl==0.4.0) (1.45.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb->axolotl==0.4.0) (0.4.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb->axolotl==0.4.0) (1.3.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb->axolotl==0.4.0) (1.4.4)\nRequirement already satisfied: botocore<1.34.70,>=1.34.41 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->axolotl==0.4.0) (1.34.69)\nRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->axolotl==0.4.0) (1.14.1)\nRequirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->axolotl==0.4.0) (0.11.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.1->axolotl==0.4.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.1->axolotl==0.4.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.1->axolotl==0.4.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.1->axolotl==0.4.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.1->axolotl==0.4.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.19.1->axolotl==0.4.0) (4.0.3)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.4.0) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.4.0) (0.12.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.4.0) (4.0.11)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs->axolotl==0.4.0) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs->axolotl==0.4.0) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.2->gcsfs->axolotl==0.4.0) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib->gcsfs->axolotl==0.4.0) (1.3.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl==0.4.0) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl==0.4.0) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl==0.4.0) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl==0.4.0) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl==0.4.0) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==3.50.2->axolotl==0.4.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.19.1->axolotl==0.4.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.19.1->axolotl==0.4.0) (2023.4)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit>=3.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0) (0.2.13)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0) (2.17.2)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.8.5->axolotl==0.4.0) (0.15)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.5->axolotl==0.4.0)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==3.50.2->axolotl==0.4.0) (0.14.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum==1.16.2->axolotl==0.4.0)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio==3.50.2->axolotl==0.4.0) (0.32.0.post1)\nRequirement already satisfied: google-api-core<3.0dev,>=1.29.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs->axolotl==0.4.0) (2.11.1)\nRequirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs->axolotl==0.4.0) (2.4.1)\nRequirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->gcsfs->axolotl==0.4.0) (2.7.0)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==3.50.2->axolotl==0.4.0) (4.2.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==3.50.2->axolotl==0.4.0) (1.0.5)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==3.50.2->axolotl==0.4.0) (1.3.0)\nCollecting wavedrom (from markdown2[all]->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0)\n  Downloading wavedrom-2.0.3.post3.tar.gz (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.7/137.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.16.2->axolotl==0.4.0) (1.3.0)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs->axolotl==0.4.0) (1.0.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.4.0) (5.0.1)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0dev,>=1.29.0->google-cloud-storage->gcsfs->axolotl==0.4.0) (1.62.0)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage->gcsfs->axolotl==0.4.0) (1.5.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.4.0) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.4.0) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.50.2->axolotl==0.4.0) (0.16.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0) (0.1.2)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->axolotl==0.4.0) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->axolotl==0.4.0) (3.2.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx->gradio==3.50.2->axolotl==0.4.0) (1.2.0)\nCollecting svgwrite (from wavedrom->markdown2[all]->fschat@ git+https://github.com/lm-sys/FastChat.git@27a05b04a35510afb1d767ae7e5990cbd278f8fe->axolotl==0.4.0)\n  Downloading svgwrite-1.4.3-py3-none-any.whl.metadata (8.8 kB)\nDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading optimum-1.16.2-py3-none-any.whl (402 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.5/402.5 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-2.6.3-py3-none-any.whl (395 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.2/395.2 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.8.5-py3-none-any.whl (245 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.1/245.1 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nDownloading art-6.2-py3-none-any.whl (601 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.8/601.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading deepspeed_kernels-0.0.1.dev1698255861-py3-none-manylinux1_x86_64.whl (44.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hf_transfer-0.1.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cmake-3.29.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading tyro-0.8.4-py3-none-any.whl (102 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nh3-0.2.17-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.1/777.1 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\nDownloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\nDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nDownloading markdown2-2.4.13-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed, flash-attn, fire, fschat, ffmpy, wavedrom\n  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432234 sha256=69a2c12f2a560950360504412c69dc77d166f777e2fad7b4e9b8f7c8d03ac1a8\n  Stored in directory: /root/.cache/pip/wheels/ea/7c/43/bed44d8414c099ff962b754f425f7ff77cc623cc8a98e0da70\n  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.8-cp310-cp310-linux_x86_64.whl size=120607435 sha256=b962f0eb38a2e54a3ece20b4b43f59f5a638ce53fe6e269992c58b119425d1f0\n  Stored in directory: /root/.cache/pip/wheels/9b/5b/2b/dea8af4e954161c49ef1941938afcd91bb93689371ed12a226\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=df1a221e7d8b4798456833b92b1f5b1976174a16d4e6eeb7cb39b60532a5ec36\n  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n  Building wheel for fschat (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fschat: filename=fschat-0.2.36-py3-none-any.whl size=272079 sha256=97946bac55f9958a55c6555d8d7d98688951b6d504b31176ff418b8e50884195\n  Stored in directory: /root/.cache/pip/wheels/21/dc/55/8647f928ab3e6390d35d3bb898acca851918560726ecdfc42a\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=c55e381dad41c45620b2da7f060725e4bd3209bff67805e81fcbe0782edc6ec1\n  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n  Building wheel for wavedrom (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30052 sha256=4dae196fedb186a38520dc1d683de9b1cc791c8b1c7a1825a98397197e7a7ff7\n  Stored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\nSuccessfully built deepspeed flash-attn fire fschat ffmpy wavedrom\nInstalling collected packages: nh3, hjson, ffmpy, addict, websockets, svgwrite, shtab, shortuuid, semantic-version, python-multipart, pydantic-core, packaging, markdown2, humanfriendly, hf_transfer, fire, einops, cmake, art, wavedrom, tiktoken, responses, pydantic, deepspeed-kernels, coloredlogs, xformers, tyro, tokenizers, gradio-client, flash-attn, deepspeed, bitsandbytes, accelerate, transformers, fschat, datasets, trl, peft, gradio, evaluate, optimum, axolotl\n  Attempting uninstall: websockets\n    Found existing installation: websockets 12.0\n    Uninstalling websockets-12.0:\n      Successfully uninstalled websockets-12.0\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.14.6\n    Uninstalling pydantic_core-2.14.6:\n      Successfully uninstalled pydantic_core-2.14.6\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.29.3\n    Uninstalling accelerate-0.29.3:\n      Successfully uninstalled accelerate-0.29.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.18.0\n    Uninstalling datasets-2.18.0:\n      Successfully uninstalled datasets-2.18.0\n  Running setup.py develop for axolotl\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.9.3 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.30.1 addict-2.4.0 art-6.2 axolotl-0.4.0 bitsandbytes-0.43.1 cmake-3.29.3 coloredlogs-15.0.1 datasets-2.19.1 deepspeed-0.14.2 deepspeed-kernels-0.0.1.dev1698255861 einops-0.8.0 evaluate-0.4.1 ffmpy-0.3.2 fire-0.6.0 flash-attn-2.5.8 fschat-0.2.36 gradio-3.50.2 gradio-client-0.6.1 hf_transfer-0.1.6 hjson-3.1.0 humanfriendly-10.0 markdown2-2.4.13 nh3-0.2.17 optimum-1.16.2 packaging-23.2 peft-0.10.0 pydantic-2.6.3 pydantic-core-2.16.3 python-multipart-0.0.9 responses-0.18.0 semantic-version-2.10.0 shortuuid-1.0.13 shtab-1.7.1 svgwrite-1.4.3 tiktoken-0.7.0 tokenizers-0.19.1 transformers-4.40.2 trl-0.8.5 tyro-0.8.4 wavedrom-2.0.3.post3 websockets-11.0.3 xformers-0.0.23.post1\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Path to the accelerate config file\nconfig_path = os.path.expanduser('~/.cache/huggingface/accelerate/default_config.yaml')\n\n# Configuration settings (modify as needed)\nconfig_content = \"\"\"\ncompute_environment: LOCAL_MACHINE\ndebug: true\ndeepspeed_config:\n  deepspeed_config_file: deepspeed_configs/zero1.json\n  zero3_init_flag: true\ndistributed_type: DEEPSPEED\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nnum_machines: 1\nnum_processes: 2\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n\"\"\"\n\n# Write the configuration file\nos.makedirs(os.path.dirname(config_path), exist_ok=True)\nwith open(config_path, 'w') as f:\n    f.write(config_content)\n\nprint(\"Accelerate config created at:\", config_path)","metadata":{"execution":{"iopub.status.busy":"2024-05-18T16:46:05.916365Z","iopub.execute_input":"2024-05-18T16:46:05.917271Z","iopub.status.idle":"2024-05-18T16:46:05.925035Z","shell.execute_reply.started":"2024-05-18T16:46:05.917231Z","shell.execute_reply":"2024-05-18T16:46:05.924109Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Accelerate config created at: /root/.cache/huggingface/accelerate/default_config.yaml\n","output_type":"stream"}]},{"cell_type":"code","source":"# Configuration text \nconfig_text = '''\nbase_model: meta-llama/Llama-2-7b-chat-hf\nmodel_type: AutoModelForCausalLM\ntokenizer_type: AutoTokenizer\n\nload_in_8bit: false\nload_in_4bit: true\nstrict: false\n\ndatasets:\n  - path: diabolic6045/Sanskrit-llama\n    type: alpaca\ndataset_prepared_path:\nval_set_size: 0\noutput_dir: ./outputs/qlora-out\nadapter: qlora\nlora_model_dir:\nsequence_len: 1024\nsample_packing: true\npad_to_sequence_len: true\neval_sample_packing: false\nlora_r: 32\nlora_alpha: 16\nlora_dropout: 0.05\nlora_target_modules:\n  - q_proj\n  - v_proj\n  - k_proj\nlora_target_linear: true\nlora_fan_in_fan_out:\n\nwandb_project:\nwandb_entity:\nwandb_watch:\nwandb_name:\nwandb_log_model:\n\ngradient_accumulation_steps: 4\nmicro_batch_size: 2\nnum_epochs: 1\noptimizer: paged_adamw_32bit\nlr_scheduler: cosine\nlearning_rate: 2e-5\n\ntrain_on_inputs: false\ngroup_by_length: false\nbf16: false\nfp16: true\ntf32: false\n\ngradient_checkpointing: true\nearly_stopping_patience:\nresume_from_checkpoint:\nlocal_rank:\nlogging_steps: 1\nxformers_attention:\nflash_attention: false\nload_best_model_at_end: false\n\nwarmup_steps: 10\nevals_per_epoch: 4\neval_table_size:\nsaves_per_epoch: 1\ndebug:\ndeepspeed: \nweight_decay: 0.0\nfsdp:\nfsdp_config:\nspecial_tokens:\n'''\n\n# File path where the .yml file should be saved\nfile_path = 'qlora.yml'\n\n# Writing the configuration to the file\nwith open(file_path, 'w') as file:\n    file.write(config_text)\n\nprint(f\"Configuration saved to {file_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:34:15.051464Z","iopub.execute_input":"2024-05-18T17:34:15.052109Z","iopub.status.idle":"2024-05-18T17:34:15.060522Z","shell.execute_reply.started":"2024-05-18T17:34:15.052063Z","shell.execute_reply":"2024-05-18T17:34:15.059466Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Configuration saved to qlora.yml\n","output_type":"stream"}]},{"cell_type":"code","source":"!accelerate launch -m axolotl.cli.train qlora.yml","metadata":{"execution":{"iopub.status.busy":"2024-05-18T17:34:17.186407Z","iopub.execute_input":"2024-05-18T17:34:17.187195Z","iopub.status.idle":"2024-05-18T17:48:31.651451Z","shell.execute_reply.started":"2024-05-18T17:34:17.187163Z","shell.execute_reply":"2024-05-18T17:48:31.650241Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"[2024-05-18 17:34:22,566] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n[2024-05-18 17:34:24,694] torch.distributed.run: [WARNING] \n[2024-05-18 17:34:24,694] torch.distributed.run: [WARNING] *****************************************\n[2024-05-18 17:34:24,694] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n[2024-05-18 17:34:24,694] torch.distributed.run: [WARNING] *****************************************\n[2024-05-18 17:34:28,909] [INFO] [datasets.<module>:58] [PID:918] PyTorch version 2.1.2 available.\n[2024-05-18 17:34:28,910] [INFO] [datasets.<module>:70] [PID:918] Polars version 0.20.21 available.\n[2024-05-18 17:34:28,911] [INFO] [datasets.<module>:105] [PID:918] TensorFlow version 2.15.0 available.\n[2024-05-18 17:34:28,913] [INFO] [datasets.<module>:118] [PID:918] JAX version 0.4.23 available.\n[2024-05-18 17:34:28,914] [INFO] [datasets.<module>:132] [PID:918] Apache Beam version 2.46.0 available.\n[2024-05-18 17:34:28,917] [INFO] [datasets.<module>:58] [PID:917] PyTorch version 2.1.2 available.\n[2024-05-18 17:34:28,918] [INFO] [datasets.<module>:70] [PID:917] Polars version 0.20.21 available.\n[2024-05-18 17:34:28,919] [INFO] [datasets.<module>:105] [PID:917] TensorFlow version 2.15.0 available.\n[2024-05-18 17:34:28,921] [INFO] [datasets.<module>:118] [PID:917] JAX version 0.4.23 available.\n[2024-05-18 17:34:28,922] [INFO] [datasets.<module>:132] [PID:917] Apache Beam version 2.46.0 available.\n2024-05-18 17:34:30.124782: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-18 17:34:30.124846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-18 17:34:30.126532: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-05-18 17:34:30.127981: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-18 17:34:30.128022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-18 17:34:30.129463: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[2024-05-18 17:34:32,981] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-05-18 17:34:33,031] [INFO] [root.spawn:38] [PID:918] gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c /tmp/tmpilnc22_k/test.c -o /tmp/tmpilnc22_k/test.o\n[2024-05-18 17:34:33,059] [INFO] [root.spawn:38] [PID:918] gcc -pthread -B /opt/conda/compiler_compat /tmp/tmpilnc22_k/test.o -laio -o /tmp/tmpilnc22_k/a.out\n\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n[2024-05-18 17:34:33,201] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2024-05-18 17:34:33,244] [INFO] [root.spawn:38] [PID:917] gcc -pthread -B /opt/conda/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -c /tmp/tmp6vfbtd0e/test.c -o /tmp/tmp6vfbtd0e/test.o\n[2024-05-18 17:34:33,270] [INFO] [root.spawn:38] [PID:917] gcc -pthread -B /opt/conda/compiler_compat /tmp/tmp6vfbtd0e/test.o -laio -o /tmp/tmp6vfbtd0e/a.out\n\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n\u001b[93m [WARNING] \u001b[0m NVIDIA Inference is only supported on Ampere and newer architectures\n\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1\n\u001b[93m [WARNING] \u001b[0m please install triton==1.0.0 if you want to use sparse attention\n\u001b[33m[2024-05-18 17:34:35,102] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_wo_flash:686] [PID:918] [RANK:1] sample_packing without flash_attention or sdp_attention does not handle cross-attention.\u001b[39m\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\u001b[33m[2024-05-18 17:34:35,266] [WARNING] [axolotl.utils.config.models.input.check_sample_packing_wo_flash:686] [PID:917] [RANK:0] sample_packing without flash_attention or sdp_attention does not handle cross-attention.\u001b[39m\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nconfig.json: 100%|█████████████████████████████| 614/614 [00:00<00:00, 4.02MB/s]\n[2024-05-18 17:34:35,284] [INFO] [axolotl.log_gpu_memory_usage:81] [PID:918] [RANK:1] GPU memory usage baseline: 0.000GB (+0.255GB misc)\u001b[39m\n[2024-05-18 17:34:35,289] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-05-18 17:34:35,361] [INFO] [axolotl.log_gpu_memory_usage:81] [PID:917] [RANK:0] GPU memory usage baseline: 0.000GB (+0.255GB misc)\u001b[39m\n[2024-05-18 17:34:35,366] [INFO] [comm.py:637:init_distributed] cdb=None\n[2024-05-18 17:34:35,366] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n                                 dP            dP   dP \n                                 88            88   88 \n      .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n      88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n      88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n      `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP \n                                                       \n                                                       \n\n****************************************\n**** Axolotl Dependency Versions *****\n  accelerate: 0.30.1         \n        peft: 0.10.0         \ntransformers: 4.40.2         \n         trl: 0.8.5          \n       torch: 2.1.2          \nbitsandbytes: 0.43.1         \n****************************************\n\u001b[33m[2024-05-18 17:34:35,405] [WARNING] [axolotl.scripts.check_accelerate_default_config:468] [PID:917] [RANK:0] accelerate config file found at /root/.cache/huggingface/accelerate/default_config.yaml. This can lead to unexpected errors\u001b[39m\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\u001b[33m[2024-05-18 17:34:35,462] [WARNING] [axolotl.scripts.check_accelerate_default_config:468] [PID:918] [RANK:1] accelerate config file found at /root/.cache/huggingface/accelerate/default_config.yaml. This can lead to unexpected errors\u001b[39m\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\ntokenizer_config.json: 100%|███████████████| 1.62k/1.62k [00:00<00:00, 10.7MB/s]\ntokenizer.model: 100%|███████████████████████| 500k/500k [00:00<00:00, 9.91MB/s]\ntokenizer.json: 100%|██████████████████████| 1.84M/1.84M [00:00<00:00, 5.60MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 414/414 [00:00<00:00, 3.96MB/s]\n[2024-05-18 17:34:36,600] [DEBUG] [axolotl.load_tokenizer:280] [PID:917] [RANK:0] EOS: 2 / </s>\u001b[39m\n[2024-05-18 17:34:36,601] [DEBUG] [axolotl.load_tokenizer:281] [PID:917] [RANK:0] BOS: 1 / <s>\u001b[39m\n[2024-05-18 17:34:36,601] [DEBUG] [axolotl.load_tokenizer:282] [PID:917] [RANK:0] PAD: 2 / </s>\u001b[39m\n[2024-05-18 17:34:36,601] [DEBUG] [axolotl.load_tokenizer:283] [PID:917] [RANK:0] UNK: 0 / <unk>\u001b[39m\n[2024-05-18 17:34:36,601] [INFO] [axolotl.load_tokenizer:294] [PID:917] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n[2024-05-18 17:34:36,601] [INFO] [axolotl.load_tokenized_prepared_datasets:183] [PID:917] [RANK:0] Unable to find prepared dataset in last_run_prepared/375a141066a9a004a08654497a115cc8\u001b[39m\n[2024-05-18 17:34:36,601] [INFO] [axolotl.load_tokenized_prepared_datasets:184] [PID:917] [RANK:0] Loading raw datasets...\u001b[39m\n\u001b[33m[2024-05-18 17:34:36,601] [WARNING] [axolotl.load_tokenized_prepared_datasets:186] [PID:917] [RANK:0] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n[2024-05-18 17:34:36,601] [INFO] [axolotl.load_tokenized_prepared_datasets:193] [PID:917] [RANK:0] No seed provided, using default seed of 42\u001b[39m\n[2024-05-18 17:34:36,609] [DEBUG] [axolotl.load_tokenizer:280] [PID:918] [RANK:1] EOS: 2 / </s>\u001b[39m\n[2024-05-18 17:34:36,610] [DEBUG] [axolotl.load_tokenizer:281] [PID:918] [RANK:1] BOS: 1 / <s>\u001b[39m\n[2024-05-18 17:34:36,610] [DEBUG] [axolotl.load_tokenizer:282] [PID:918] [RANK:1] PAD: 2 / </s>\u001b[39m\n[2024-05-18 17:34:36,610] [DEBUG] [axolotl.load_tokenizer:283] [PID:918] [RANK:1] UNK: 0 / <unk>\u001b[39m\n[2024-05-18 17:34:36,610] [INFO] [axolotl.load_tokenizer:294] [PID:918] [RANK:1] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\nTokenizing Prompts (num_proc=4):  59%|▌| 11938/20227 [00:04<00:03, 2551.64 examp\u001b[33m[2024-05-18 17:34:43,349] [WARNING] [axolotl._tokenize:66] [PID:985] [RANK:0] Empty text requested for tokenization.\u001b[39m\nTokenizing Prompts (num_proc=4):  60%|▌| 12233/20227 [00:04<00:03, 2159.33 examp\u001b[33m[2024-05-18 17:34:43,431] [WARNING] [axolotl._tokenize:66] [PID:985] [RANK:0] Empty text requested for tokenization.\u001b[39m\nTokenizing Prompts (num_proc=4):  62%|▌| 12506/20227 [00:04<00:03, 2065.42 examp\u001b[33m[2024-05-18 17:34:43,565] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:43,568] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:43,570] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\nTokenizing Prompts (num_proc=4):  69%|▋| 13857/20227 [00:05<00:02, 2958.98 examp\u001b[33m[2024-05-18 17:34:44,022] [WARNING] [axolotl._tokenize:66] [PID:985] [RANK:0] Empty text requested for tokenization.\u001b[39m\nTokenizing Prompts (num_proc=4):  76%|▊| 15295/20227 [00:05<00:01, 3397.93 examp\u001b[33m[2024-05-18 17:34:44,405] [WARNING] [axolotl._tokenize:66] [PID:985] [RANK:0] Empty text requested for tokenization.\u001b[39m\nTokenizing Prompts (num_proc=4):  81%|▊| 16301/20227 [00:05<00:01, 2266.48 examp\u001b[33m[2024-05-18 17:34:44,889] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,890] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,891] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,892] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,893] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,894] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,895] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,896] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,897] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,898] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,899] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,900] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,927] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\n\u001b[33m[2024-05-18 17:34:44,928] [WARNING] [axolotl._tokenize:66] [PID:982] [RANK:0] Empty text requested for tokenization.\u001b[39m\nTokenizing Prompts (num_proc=4):  99%|▉| 20057/20227 [00:07<00:00, 1907.61 examp\u001b[33m[2024-05-18 17:34:46,284] [WARNING] [axolotl._tokenize:66] [PID:985] [RANK:0] Empty text requested for tokenization.\u001b[39m\nTokenizing Prompts (num_proc=4): 100%|█| 20227/20227 [00:07<00:00, 2676.35 examp\n[2024-05-18 17:34:46,499] [INFO] [axolotl.load_tokenized_prepared_datasets:410] [PID:917] [RANK:0] merging datasets\u001b[39m\nDropping Long Sequences (num_proc=4): 100%|█| 20227/20227 [00:04<00:00, 4806.46 \nAdd position_id column (Sample Packing) (num_proc=4): 100%|█| 20221/20221 [00:03\n[2024-05-18 17:34:56,244] [INFO] [axolotl.load_tokenized_prepared_datasets:183] [PID:918] [RANK:1] Unable to find prepared dataset in last_run_prepared/375a141066a9a004a08654497a115cc8\u001b[39m\n[2024-05-18 17:34:56,245] [INFO] [axolotl.load_tokenized_prepared_datasets:184] [PID:918] [RANK:1] Loading raw datasets...\u001b[39m\n[2024-05-18 17:34:56,245] [INFO] [axolotl.load_tokenized_prepared_datasets:423] [PID:917] [RANK:0] Saving merged prepared dataset to disk... last_run_prepared/375a141066a9a004a08654497a115cc8\u001b[39m\n\u001b[33m[2024-05-18 17:34:56,245] [WARNING] [axolotl.load_tokenized_prepared_datasets:186] [PID:918] [RANK:1] Processing datasets during training can lead to VRAM instability. Please pre-process your dataset.\u001b[39m\n[2024-05-18 17:34:56,245] [INFO] [axolotl.load_tokenized_prepared_datasets:193] [PID:918] [RANK:1] No seed provided, using default seed of 42\u001b[39m\nSaving the dataset (1/1 shards): 100%|█| 20221/20221 [00:00<00:00, 149352.09 exa\n[2024-05-18 17:34:58,340] [INFO] [axolotl.load_tokenized_prepared_datasets:410] [PID:918] [RANK:1] merging datasets\u001b[39m\n[2024-05-18 17:34:58,395] [DEBUG] [axolotl.log:63] [PID:917] [RANK:0] total_num_tokens: 5_275_486\u001b[39m\n[2024-05-18 17:34:58,708] [DEBUG] [axolotl.log:63] [PID:917] [RANK:0] `total_supervised_tokens: 1_019_738`\u001b[39m\n[2024-05-18 17:35:04,748] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:918] [RANK:1] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:35:04,915] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:917] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:35:04,916] [DEBUG] [axolotl.log:63] [PID:917] [RANK:0] data_loader_len: 318\u001b[39m\n[2024-05-18 17:35:04,968] [INFO] [axolotl.log:63] [PID:917] [RANK:0] sample_packing_eff_est across ranks: [0.8625216484069824, 0.8615120053291321]\u001b[39m\n[2024-05-18 17:35:04,968] [DEBUG] [axolotl.log:63] [PID:917] [RANK:0] sample_packing_eff_est: 0.87\u001b[39m\n[2024-05-18 17:35:04,968] [DEBUG] [axolotl.log:63] [PID:917] [RANK:0] total_num_steps: 318\u001b[39m\n[2024-05-18 17:35:04,969] [DEBUG] [axolotl.train.log:63] [PID:917] [RANK:0] loading tokenizer... meta-llama/Llama-2-7b-chat-hf\u001b[39m\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n[2024-05-18 17:35:05,185] [DEBUG] [axolotl.load_tokenizer:280] [PID:917] [RANK:0] EOS: 2 / </s>\u001b[39m\n[2024-05-18 17:35:05,186] [DEBUG] [axolotl.load_tokenizer:281] [PID:917] [RANK:0] BOS: 1 / <s>\u001b[39m\n[2024-05-18 17:35:05,186] [DEBUG] [axolotl.load_tokenizer:282] [PID:917] [RANK:0] PAD: 2 / </s>\u001b[39m\n[2024-05-18 17:35:05,186] [DEBUG] [axolotl.load_tokenizer:283] [PID:917] [RANK:0] UNK: 0 / <unk>\u001b[39m\n[2024-05-18 17:35:05,186] [INFO] [axolotl.load_tokenizer:294] [PID:917] [RANK:0] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n[2024-05-18 17:35:05,186] [DEBUG] [axolotl.train.log:63] [PID:917] [RANK:0] loading model and peft_config...\u001b[39m\n/opt/conda/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:832: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\n  warnings.warn(\"DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\")\n[2024-05-18 17:35:05,220] [DEBUG] [axolotl.load_tokenizer:280] [PID:918] [RANK:1] EOS: 2 / </s>\u001b[39m\n[2024-05-18 17:35:05,220] [DEBUG] [axolotl.load_tokenizer:281] [PID:918] [RANK:1] BOS: 1 / <s>\u001b[39m\n[2024-05-18 17:35:05,220] [DEBUG] [axolotl.load_tokenizer:282] [PID:918] [RANK:1] PAD: 2 / </s>\u001b[39m\n[2024-05-18 17:35:05,220] [DEBUG] [axolotl.load_tokenizer:283] [PID:918] [RANK:1] UNK: 0 / <unk>\u001b[39m\n[2024-05-18 17:35:05,220] [INFO] [axolotl.load_tokenizer:294] [PID:918] [RANK:1] No Chat template selected. Consider adding a chat template for easier inference.\u001b[39m\n/opt/conda/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:832: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\n  warnings.warn(\"DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\")\n[2024-05-18 17:35:05,279] [INFO] [axolotl.load_model:386] [PID:918] [RANK:1] patching llama _prepare_4d_causal_attention_mask*\u001b[39m\n[2024-05-18 17:35:05,283] [INFO] [axolotl.load_model:386] [PID:917] [RANK:0] patching llama _prepare_4d_causal_attention_mask*\u001b[39m\n[2024-05-18 17:35:05,285] [INFO] [axolotl.load_model:409] [PID:918] [RANK:1] patching _expand_mask\u001b[39m\n[2024-05-18 17:35:05,288] [INFO] [axolotl.load_model:409] [PID:917] [RANK:0] patching _expand_mask\u001b[39m\nmodel.safetensors.index.json: 100%|████████| 26.8k/26.8k [00:00<00:00, 24.3MB/s]\nDownloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\nmodel-00001-of-00002.safetensors:   0%|             | 0.00/9.98G [00:00<?, ?B/s]\u001b[A\nmodel-00001-of-00002.safetensors:   0%|    | 10.5M/9.98G [00:00<01:42, 97.2MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   0%|     | 41.9M/9.98G [00:00<00:49, 199MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|     | 73.4M/9.98G [00:00<00:40, 242MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|      | 105M/9.98G [00:00<00:37, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   1%|      | 136M/9.98G [00:00<00:35, 275MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|      | 168M/9.98G [00:00<00:34, 287MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|      | 199M/9.98G [00:00<00:33, 294MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   2%|▏     | 231M/9.98G [00:00<00:33, 291MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏     | 262M/9.98G [00:00<00:33, 289MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏     | 294M/9.98G [00:01<00:33, 289MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   3%|▏     | 325M/9.98G [00:01<00:33, 284MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏     | 357M/9.98G [00:01<00:34, 281MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▏     | 388M/9.98G [00:01<00:34, 281MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   4%|▎     | 419M/9.98G [00:01<00:33, 284MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎     | 451M/9.98G [00:01<00:33, 283MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎     | 482M/9.98G [00:01<00:33, 286MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎     | 514M/9.98G [00:01<00:32, 290MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   5%|▎     | 545M/9.98G [00:01<00:32, 290MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎     | 577M/9.98G [00:02<00:32, 289MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▎     | 608M/9.98G [00:02<00:32, 285MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   6%|▍     | 640M/9.98G [00:02<00:32, 288MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▍     | 671M/9.98G [00:02<00:32, 286MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▍     | 703M/9.98G [00:02<00:31, 291MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   7%|▍     | 734M/9.98G [00:02<00:31, 291MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍     | 765M/9.98G [00:02<00:32, 287MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍     | 797M/9.98G [00:02<00:31, 290MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   8%|▍     | 828M/9.98G [00:02<00:31, 292MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▌     | 860M/9.98G [00:03<00:30, 297MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▌     | 891M/9.98G [00:03<00:30, 300MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:   9%|▌     | 923M/9.98G [00:03<00:30, 298MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▌     | 954M/9.98G [00:03<00:30, 300MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▌     | 986M/9.98G [00:03<00:29, 303MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  10%|▌    | 1.02G/9.98G [00:03<00:29, 306MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 1.05G/9.98G [00:03<00:29, 305MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 1.08G/9.98G [00:03<00:29, 305MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 1.11G/9.98G [00:03<00:29, 303MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  11%|▌    | 1.14G/9.98G [00:03<00:28, 305MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 1.17G/9.98G [00:04<00:28, 307MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 1.21G/9.98G [00:04<00:28, 305MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  12%|▌    | 1.24G/9.98G [00:04<00:28, 306MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 1.28G/9.98G [00:04<00:28, 307MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 1.31G/9.98G [00:04<00:28, 303MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  13%|▋    | 1.34G/9.98G [00:04<00:29, 297MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 1.37G/9.98G [00:04<00:29, 293MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 1.41G/9.98G [00:04<00:29, 294MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  14%|▋    | 1.44G/9.98G [00:04<00:29, 294MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▋    | 1.47G/9.98G [00:05<00:29, 293MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▊    | 1.50G/9.98G [00:05<00:28, 295MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  15%|▊    | 1.53G/9.98G [00:05<00:28, 295MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 1.56G/9.98G [00:05<00:29, 289MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 1.59G/9.98G [00:05<00:29, 284MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  16%|▊    | 1.63G/9.98G [00:05<00:29, 281MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 1.66G/9.98G [00:05<00:29, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 1.69G/9.98G [00:05<00:30, 276MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  17%|▊    | 1.72G/9.98G [00:05<00:29, 276MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 1.75G/9.98G [00:06<00:29, 277MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 1.78G/9.98G [00:06<00:29, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 1.81G/9.98G [00:06<00:29, 276MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  18%|▉    | 1.85G/9.98G [00:06<00:30, 271MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 1.88G/9.98G [00:06<00:30, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 1.91G/9.98G [00:06<00:30, 264MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  19%|▉    | 1.94G/9.98G [00:06<00:30, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|▉    | 1.97G/9.98G [00:06<00:30, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|█    | 2.00G/9.98G [00:07<00:30, 261MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  20%|█    | 2.03G/9.98G [00:07<00:30, 261MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 2.07G/9.98G [00:07<00:30, 263MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 2.10G/9.98G [00:07<00:30, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  21%|█    | 2.13G/9.98G [00:07<00:28, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 2.16G/9.98G [00:07<00:28, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 2.19G/9.98G [00:07<00:27, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  22%|█    | 2.22G/9.98G [00:07<00:27, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 2.25G/9.98G [00:07<00:26, 287MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 2.29G/9.98G [00:08<00:26, 292MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  23%|█▏   | 2.32G/9.98G [00:08<00:26, 294MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 2.35G/9.98G [00:08<00:25, 294MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 2.38G/9.98G [00:08<00:26, 289MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 2.41G/9.98G [00:08<00:26, 283MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  24%|█▏   | 2.44G/9.98G [00:08<00:27, 277MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▏   | 2.47G/9.98G [00:08<00:27, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▎   | 2.51G/9.98G [00:08<00:27, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  25%|█▎   | 2.54G/9.98G [00:08<00:27, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█▎   | 2.57G/9.98G [00:09<00:27, 274MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█▎   | 2.60G/9.98G [00:09<00:26, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  26%|█▎   | 2.63G/9.98G [00:09<00:27, 271MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█▎   | 2.66G/9.98G [00:09<00:26, 279MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█▎   | 2.69G/9.98G [00:09<00:25, 284MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  27%|█▎   | 2.73G/9.98G [00:09<00:25, 288MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▍   | 2.77G/9.98G [00:09<00:24, 297MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▍   | 2.80G/9.98G [00:09<00:23, 300MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  28%|█▍   | 2.83G/9.98G [00:09<00:24, 296MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▍   | 2.86G/9.98G [00:10<00:23, 298MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▍   | 2.89G/9.98G [00:10<00:23, 299MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  29%|█▍   | 2.93G/9.98G [00:10<00:23, 301MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▍   | 2.96G/9.98G [00:10<00:23, 302MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▌   | 3.00G/9.98G [00:10<00:22, 304MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  30%|█▌   | 3.04G/9.98G [00:10<00:22, 309MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▌   | 3.07G/9.98G [00:10<00:22, 308MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▌   | 3.10G/9.98G [00:10<00:22, 304MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  31%|█▌   | 3.14G/9.98G [00:10<00:22, 303MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▌   | 3.17G/9.98G [00:11<00:22, 301MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▌   | 3.20G/9.98G [00:11<00:22, 302MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  32%|█▌   | 3.23G/9.98G [00:11<00:22, 301MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▋   | 3.26G/9.98G [00:11<00:22, 298MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▋   | 3.29G/9.98G [00:11<00:22, 297MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  33%|█▋   | 3.32G/9.98G [00:11<00:22, 296MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▋   | 3.36G/9.98G [00:11<00:22, 294MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▋   | 3.39G/9.98G [00:11<00:22, 295MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  34%|█▋   | 3.42G/9.98G [00:11<00:22, 295MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▋   | 3.45G/9.98G [00:12<00:22, 296MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▋   | 3.48G/9.98G [00:12<00:21, 295MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  35%|█▊   | 3.51G/9.98G [00:12<00:21, 297MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▊   | 3.54G/9.98G [00:12<00:21, 300MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▊   | 3.58G/9.98G [00:12<00:21, 303MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▊   | 3.61G/9.98G [00:12<00:21, 302MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  36%|█▊   | 3.64G/9.98G [00:12<00:21, 301MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▊   | 3.67G/9.98G [00:12<00:21, 299MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▊   | 3.70G/9.98G [00:12<00:20, 299MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  37%|█▊   | 3.73G/9.98G [00:12<00:20, 300MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▉   | 3.76G/9.98G [00:13<00:20, 301MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▉   | 3.80G/9.98G [00:13<00:20, 302MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  38%|█▉   | 3.83G/9.98G [00:13<00:20, 306MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▉   | 3.86G/9.98G [00:13<00:20, 306MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▉   | 3.89G/9.98G [00:13<00:19, 304MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  39%|█▉   | 3.92G/9.98G [00:13<00:20, 299MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▉   | 3.95G/9.98G [00:13<00:20, 296MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|█▉   | 3.98G/9.98G [00:13<00:20, 285MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  40%|██   | 4.02G/9.98G [00:13<00:21, 282MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|██   | 4.05G/9.98G [00:14<00:21, 282MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|██   | 4.08G/9.98G [00:14<00:21, 279MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  41%|██   | 4.11G/9.98G [00:14<00:21, 276MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██   | 4.14G/9.98G [00:14<00:21, 275MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██   | 4.17G/9.98G [00:14<00:21, 274MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██   | 4.20G/9.98G [00:14<00:21, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  42%|██   | 4.24G/9.98G [00:14<00:21, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|██▏  | 4.27G/9.98G [00:14<00:21, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|██▏  | 4.30G/9.98G [00:14<00:21, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  43%|██▏  | 4.33G/9.98G [00:15<00:20, 276MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|██▏  | 4.36G/9.98G [00:15<00:20, 274MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|██▏  | 4.39G/9.98G [00:15<00:20, 275MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  44%|██▏  | 4.42G/9.98G [00:15<00:20, 274MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|██▏  | 4.46G/9.98G [00:15<00:20, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|██▏  | 4.49G/9.98G [00:15<00:20, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  45%|██▎  | 4.52G/9.98G [00:15<00:20, 269MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|██▎  | 4.55G/9.98G [00:15<00:20, 269MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|██▎  | 4.58G/9.98G [00:16<00:19, 271MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  46%|██▎  | 4.61G/9.98G [00:16<00:19, 269MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|██▎  | 4.65G/9.98G [00:16<00:19, 271MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|██▎  | 4.68G/9.98G [00:16<00:19, 271MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  47%|██▎  | 4.71G/9.98G [00:16<00:19, 271MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|██▍  | 4.74G/9.98G [00:16<00:19, 275MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|██▍  | 4.77G/9.98G [00:16<00:18, 275MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|██▍  | 4.80G/9.98G [00:16<00:18, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  48%|██▍  | 4.83G/9.98G [00:16<00:18, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|██▍  | 4.87G/9.98G [00:17<00:19, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|██▍  | 4.90G/9.98G [00:17<00:19, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  49%|██▍  | 4.93G/9.98G [00:17<00:19, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██▍  | 4.96G/9.98G [00:17<00:19, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██▌  | 4.99G/9.98G [00:17<00:18, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  50%|██▌  | 5.02G/9.98G [00:17<00:18, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██▌  | 5.05G/9.98G [00:17<00:18, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██▌  | 5.09G/9.98G [00:17<00:18, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  51%|██▌  | 5.12G/9.98G [00:18<00:18, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██▌  | 5.15G/9.98G [00:18<00:17, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██▌  | 5.18G/9.98G [00:18<00:17, 274MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  52%|██▌  | 5.21G/9.98G [00:18<00:26, 177MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▋  | 5.24G/9.98G [00:18<00:23, 200MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▋  | 5.27G/9.98G [00:18<00:21, 220MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▋  | 5.31G/9.98G [00:18<00:20, 233MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  53%|██▋  | 5.34G/9.98G [00:19<00:19, 241MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▋  | 5.37G/9.98G [00:19<00:18, 252MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▋  | 5.40G/9.98G [00:19<00:17, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  54%|██▋  | 5.43G/9.98G [00:19<00:17, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▋  | 5.46G/9.98G [00:19<00:16, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▊  | 5.49G/9.98G [00:19<00:17, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  55%|██▊  | 5.53G/9.98G [00:19<00:20, 213MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▊  | 5.56G/9.98G [00:19<00:20, 211MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▊  | 5.59G/9.98G [00:20<00:19, 222MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  56%|██▊  | 5.62G/9.98G [00:20<00:18, 237MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▊  | 5.65G/9.98G [00:20<00:17, 245MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▊  | 5.68G/9.98G [00:20<00:16, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  57%|██▊  | 5.71G/9.98G [00:20<00:16, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▉  | 5.75G/9.98G [00:20<00:16, 254MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▉  | 5.78G/9.98G [00:20<00:16, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  58%|██▉  | 5.81G/9.98G [00:20<00:16, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▉  | 5.84G/9.98G [00:21<00:16, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▉  | 5.87G/9.98G [00:21<00:15, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▉  | 5.90G/9.98G [00:21<00:15, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  59%|██▉  | 5.93G/9.98G [00:21<00:16, 252MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|██▉  | 5.97G/9.98G [00:21<00:15, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|███  | 6.00G/9.98G [00:21<00:15, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  60%|███  | 6.03G/9.98G [00:21<00:14, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|███  | 6.06G/9.98G [00:21<00:14, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|███  | 6.09G/9.98G [00:21<00:13, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  61%|███  | 6.12G/9.98G [00:22<00:14, 274MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|███  | 6.16G/9.98G [00:22<00:14, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|███  | 6.19G/9.98G [00:22<00:13, 279MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  62%|███  | 6.22G/9.98G [00:22<00:13, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|███▏ | 6.25G/9.98G [00:22<00:13, 279MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|███▏ | 6.28G/9.98G [00:22<00:13, 276MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  63%|███▏ | 6.31G/9.98G [00:22<00:13, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|███▏ | 6.34G/9.98G [00:22<00:13, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|███▏ | 6.38G/9.98G [00:23<00:13, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  64%|███▏ | 6.41G/9.98G [00:23<00:13, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|███▏ | 6.44G/9.98G [00:23<00:12, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|███▏ | 6.47G/9.98G [00:23<00:12, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|███▎ | 6.50G/9.98G [00:23<00:12, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  65%|███▎ | 6.53G/9.98G [00:23<00:12, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|███▎ | 6.56G/9.98G [00:23<00:12, 271MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|███▎ | 6.60G/9.98G [00:23<00:12, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  66%|███▎ | 6.63G/9.98G [00:23<00:12, 276MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|███▎ | 6.66G/9.98G [00:24<00:11, 281MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|███▎ | 6.69G/9.98G [00:24<00:11, 279MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  67%|███▎ | 6.72G/9.98G [00:24<00:11, 282MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|███▍ | 6.75G/9.98G [00:24<00:11, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|███▍ | 6.78G/9.98G [00:24<00:11, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  68%|███▍ | 6.82G/9.98G [00:24<00:11, 269MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|███▍ | 6.85G/9.98G [00:24<00:11, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|███▍ | 6.88G/9.98G [00:24<00:11, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  69%|███▍ | 6.91G/9.98G [00:24<00:11, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|███▍ | 6.94G/9.98G [00:25<00:11, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|███▍ | 6.97G/9.98G [00:25<00:11, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  70%|███▌ | 7.00G/9.98G [00:25<00:11, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|███▌ | 7.04G/9.98G [00:25<00:10, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|███▌ | 7.07G/9.98G [00:25<00:10, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|███▌ | 7.10G/9.98G [00:25<00:10, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  71%|███▌ | 7.13G/9.98G [00:25<00:10, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|███▌ | 7.16G/9.98G [00:25<00:10, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|███▌ | 7.19G/9.98G [00:26<00:10, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  72%|███▌ | 7.22G/9.98G [00:26<00:10, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|███▋ | 7.26G/9.98G [00:26<00:10, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|███▋ | 7.29G/9.98G [00:26<00:09, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  73%|███▋ | 7.32G/9.98G [00:26<00:09, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  74%|███▋ | 7.35G/9.98G [00:26<00:09, 278MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  74%|███▋ | 7.38G/9.98G [00:26<00:09, 276MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  74%|███▋ | 7.41G/9.98G [00:26<00:09, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███▋ | 7.44G/9.98G [00:26<00:09, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███▋ | 7.48G/9.98G [00:27<00:09, 263MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  75%|███▊ | 7.51G/9.98G [00:27<00:09, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███▊ | 7.54G/9.98G [00:27<00:08, 273MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███▊ | 7.57G/9.98G [00:27<00:08, 281MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  76%|███▊ | 7.60G/9.98G [00:27<00:08, 285MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███▊ | 7.63G/9.98G [00:27<00:08, 285MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███▊ | 7.67G/9.98G [00:27<00:08, 287MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███▊ | 7.70G/9.98G [00:27<00:07, 289MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  77%|███▊ | 7.73G/9.98G [00:27<00:07, 290MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███▉ | 7.76G/9.98G [00:28<00:08, 255MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███▉ | 7.79G/9.98G [00:28<00:10, 211MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  78%|███▉ | 7.82G/9.98G [00:28<00:11, 187MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▉ | 7.85G/9.98G [00:28<00:10, 200MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▉ | 7.89G/9.98G [00:28<00:09, 209MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  79%|███▉ | 7.92G/9.98G [00:28<00:09, 219MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▉ | 7.95G/9.98G [00:29<00:08, 232MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▏| 7.98G/9.98G [00:30<00:39, 51.0MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  80%|███▏| 8.01G/9.98G [00:30<00:29, 66.5MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|███▏| 8.04G/9.98G [00:31<00:22, 86.3MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|████ | 8.07G/9.98G [00:31<00:17, 109MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  81%|████ | 8.11G/9.98G [00:31<00:14, 132MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  82%|████ | 8.14G/9.98G [00:31<00:11, 156MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  82%|████ | 8.17G/9.98G [00:31<00:10, 181MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  82%|████ | 8.20G/9.98G [00:31<00:08, 201MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|████▏| 8.23G/9.98G [00:31<00:08, 216MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|████▏| 8.26G/9.98G [00:31<00:07, 227MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|████▏| 8.29G/9.98G [00:31<00:07, 236MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  83%|████▏| 8.33G/9.98G [00:32<00:06, 243MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|████▏| 8.36G/9.98G [00:32<00:06, 248MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|████▏| 8.39G/9.98G [00:32<00:06, 252MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  84%|████▏| 8.42G/9.98G [00:32<00:06, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|████▏| 8.45G/9.98G [00:32<00:05, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|████▎| 8.48G/9.98G [00:32<00:05, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  85%|████▎| 8.51G/9.98G [00:32<00:05, 266MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|████▎| 8.55G/9.98G [00:32<00:05, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|████▎| 8.58G/9.98G [00:33<00:05, 268MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  86%|████▎| 8.61G/9.98G [00:33<00:05, 265MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  87%|████▎| 8.64G/9.98G [00:33<00:04, 270MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  87%|████▎| 8.67G/9.98G [00:33<00:04, 277MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  87%|████▎| 8.70G/9.98G [00:33<00:04, 284MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 8.73G/9.98G [00:33<00:04, 280MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 8.77G/9.98G [00:33<00:04, 269MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 8.80G/9.98G [00:33<00:05, 217MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  88%|████▍| 8.83G/9.98G [00:34<00:05, 223MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|████▍| 8.86G/9.98G [00:34<00:04, 232MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|████▍| 8.89G/9.98G [00:34<00:04, 240MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  89%|████▍| 8.92G/9.98G [00:34<00:04, 248MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|████▍| 8.95G/9.98G [00:34<00:04, 249MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|████▌| 8.99G/9.98G [00:34<00:03, 248MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  90%|████▌| 9.02G/9.98G [00:34<00:03, 253MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|████▌| 9.05G/9.98G [00:34<00:03, 252MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|████▌| 9.08G/9.98G [00:35<00:03, 249MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  91%|████▌| 9.11G/9.98G [00:35<00:03, 248MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|████▌| 9.14G/9.98G [00:35<00:03, 248MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|████▌| 9.18G/9.98G [00:35<00:03, 250MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  92%|████▌| 9.21G/9.98G [00:35<00:02, 257MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|████▋| 9.24G/9.98G [00:35<00:02, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|████▋| 9.27G/9.98G [00:35<00:02, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  93%|████▋| 9.30G/9.98G [00:35<00:02, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|████▋| 9.33G/9.98G [00:35<00:02, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|████▋| 9.36G/9.98G [00:36<00:02, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|████▋| 9.40G/9.98G [00:36<00:02, 256MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  94%|████▋| 9.43G/9.98G [00:36<00:02, 255MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|████▋| 9.46G/9.98G [00:36<00:02, 255MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|████▊| 9.49G/9.98G [00:36<00:01, 259MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  95%|████▊| 9.52G/9.98G [00:36<00:01, 253MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|████▊| 9.55G/9.98G [00:36<00:01, 250MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|████▊| 9.58G/9.98G [00:36<00:01, 251MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  96%|████▊| 9.62G/9.98G [00:37<00:01, 251MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  97%|████▊| 9.65G/9.98G [00:37<00:01, 255MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  97%|████▊| 9.68G/9.98G [00:37<00:01, 258MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  97%|████▊| 9.71G/9.98G [00:37<00:01, 260MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|████▉| 9.74G/9.98G [00:37<00:00, 259MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|████▉| 9.77G/9.98G [00:37<00:00, 257MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  98%|████▉| 9.80G/9.98G [00:37<00:00, 262MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|████▉| 9.84G/9.98G [00:37<00:00, 267MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|████▉| 9.87G/9.98G [00:38<00:00, 271MB/s]\u001b[A\nmodel-00001-of-00002.safetensors:  99%|████▉| 9.90G/9.98G [00:38<00:00, 274MB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|████▉| 9.93G/9.98G [00:38<00:00, 272MB/s]\u001b[A\nmodel-00001-of-00002.safetensors: 100%|█████| 9.98G/9.98G [00:38<00:00, 259MB/s]\u001b[A\nDownloading shards:  50%|████████████▌            | 1/2 [00:38<00:38, 38.71s/it]\nmodel-00002-of-00002.safetensors:   0%|             | 0.00/3.50G [00:00<?, ?B/s]\u001b[A\nmodel-00002-of-00002.safetensors:   1%|     | 31.5M/3.50G [00:00<00:13, 254MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   2%|     | 62.9M/3.50G [00:00<00:12, 275MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   3%|▏    | 94.4M/3.50G [00:00<00:12, 283MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   4%|▏     | 126M/3.50G [00:00<00:11, 283MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   4%|▎     | 157M/3.50G [00:00<00:11, 284MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   5%|▎     | 189M/3.50G [00:00<00:11, 283MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   6%|▍     | 220M/3.50G [00:00<00:11, 280MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   7%|▍     | 252M/3.50G [00:00<00:11, 278MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   8%|▍     | 283M/3.50G [00:01<00:11, 277MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:   9%|▌     | 315M/3.50G [00:01<00:11, 276MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  10%|▌     | 346M/3.50G [00:01<00:11, 278MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  11%|▋     | 377M/3.50G [00:01<00:11, 278MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  12%|▋     | 409M/3.50G [00:01<00:11, 276MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  13%|▊     | 440M/3.50G [00:01<00:11, 278MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  13%|▊     | 472M/3.50G [00:01<00:10, 279MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  14%|▊     | 503M/3.50G [00:01<00:10, 286MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  15%|▉     | 535M/3.50G [00:01<00:10, 290MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  16%|▉     | 566M/3.50G [00:02<00:09, 295MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  17%|█     | 598M/3.50G [00:02<00:09, 298MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  18%|█     | 629M/3.50G [00:02<00:09, 297MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  19%|█▏    | 661M/3.50G [00:02<00:09, 300MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  20%|█▏    | 692M/3.50G [00:02<00:09, 299MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  21%|█▎    | 734M/3.50G [00:02<00:09, 306MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  22%|█▎    | 765M/3.50G [00:02<00:09, 298MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  23%|█▎    | 797M/3.50G [00:02<00:09, 294MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  24%|█▍    | 828M/3.50G [00:02<00:09, 287MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  25%|█▍    | 860M/3.50G [00:03<00:09, 281MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  25%|█▌    | 891M/3.50G [00:03<00:09, 275MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  26%|█▌    | 923M/3.50G [00:03<00:09, 273MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  27%|█▋    | 954M/3.50G [00:03<00:09, 272MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  28%|█▋    | 986M/3.50G [00:03<00:09, 271MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  29%|█▍   | 1.02G/3.50G [00:03<00:09, 272MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  30%|█▍   | 1.05G/3.50G [00:03<00:09, 271MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  31%|█▌   | 1.08G/3.50G [00:03<00:08, 270MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  32%|█▌   | 1.11G/3.50G [00:03<00:08, 270MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  33%|█▋   | 1.14G/3.50G [00:04<00:08, 269MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  34%|█▋   | 1.17G/3.50G [00:04<00:08, 268MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  34%|█▋   | 1.21G/3.50G [00:04<00:11, 204MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  35%|█▊   | 1.24G/3.50G [00:04<00:14, 158MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  36%|█▊   | 1.26G/3.50G [00:04<00:15, 142MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  37%|█▊   | 1.28G/3.50G [00:05<00:16, 133MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  37%|█▊   | 1.30G/3.50G [00:05<00:17, 128MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  38%|█▉   | 1.32G/3.50G [00:05<00:21, 100MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  39%|█▉   | 1.35G/3.50G [00:05<00:16, 130MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  40%|█▉   | 1.38G/3.50G [00:05<00:14, 148MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  40%|██   | 1.41G/3.50G [00:06<00:14, 146MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  41%|██   | 1.43G/3.50G [00:06<00:14, 145MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  41%|██   | 1.45G/3.50G [00:06<00:13, 151MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  42%|██   | 1.47G/3.50G [00:06<00:13, 151MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  43%|██▏  | 1.49G/3.50G [00:06<00:12, 156MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  43%|██▏  | 1.51G/3.50G [00:06<00:11, 167MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  44%|██▏  | 1.54G/3.50G [00:06<00:10, 194MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  45%|██▏  | 1.57G/3.50G [00:06<00:09, 209MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  46%|██▎  | 1.60G/3.50G [00:07<00:08, 227MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  47%|██▎  | 1.64G/3.50G [00:07<00:07, 238MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  48%|██▍  | 1.67G/3.50G [00:07<00:07, 240MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  49%|██▍  | 1.70G/3.50G [00:07<00:07, 233MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  49%|██▍  | 1.73G/3.50G [00:07<00:07, 243MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  50%|██▌  | 1.76G/3.50G [00:07<00:06, 255MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  51%|██▌  | 1.79G/3.50G [00:07<00:06, 248MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  52%|██▌  | 1.82G/3.50G [00:07<00:06, 249MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  53%|██▋  | 1.86G/3.50G [00:08<00:06, 253MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  54%|██▋  | 1.89G/3.50G [00:08<00:06, 242MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  55%|██▋  | 1.92G/3.50G [00:08<00:07, 207MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  56%|██▊  | 1.95G/3.50G [00:08<00:08, 178MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  56%|██▊  | 1.97G/3.50G [00:08<00:09, 165MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  57%|██▊  | 1.99G/3.50G [00:08<00:09, 158MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  58%|██▉  | 2.01G/3.50G [00:09<00:10, 147MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  58%|██▉  | 2.03G/3.50G [00:09<00:10, 137MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  59%|██▉  | 2.06G/3.50G [00:09<00:10, 135MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  59%|██▉  | 2.08G/3.50G [00:09<00:09, 144MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  60%|██▉  | 2.10G/3.50G [00:09<00:09, 155MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  61%|███  | 2.13G/3.50G [00:09<00:07, 175MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  62%|███  | 2.16G/3.50G [00:09<00:06, 194MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  63%|███▏ | 2.19G/3.50G [00:10<00:06, 208MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  64%|███▏ | 2.22G/3.50G [00:10<00:06, 199MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  64%|███▏ | 2.25G/3.50G [00:10<00:05, 212MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  65%|███▎ | 2.29G/3.50G [00:10<00:05, 220MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  66%|███▎ | 2.32G/3.50G [00:10<00:05, 224MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  67%|███▎ | 2.35G/3.50G [00:10<00:05, 227MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  68%|███▍ | 2.38G/3.50G [00:10<00:04, 234MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  69%|███▍ | 2.41G/3.50G [00:11<00:04, 237MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  70%|███▍ | 2.44G/3.50G [00:11<00:04, 247MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  71%|███▌ | 2.47G/3.50G [00:11<00:04, 249MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  72%|███▌ | 2.51G/3.50G [00:11<00:04, 246MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  72%|███▌ | 2.54G/3.50G [00:11<00:03, 243MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  73%|███▋ | 2.57G/3.50G [00:11<00:03, 238MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  74%|███▋ | 2.60G/3.50G [00:11<00:03, 237MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  75%|███▊ | 2.63G/3.50G [00:11<00:03, 236MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  76%|███▊ | 2.66G/3.50G [00:12<00:03, 235MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  77%|███▊ | 2.69G/3.50G [00:12<00:03, 236MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  78%|███▉ | 2.73G/3.50G [00:12<00:03, 233MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  79%|███▉ | 2.76G/3.50G [00:12<00:03, 236MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  80%|███▉ | 2.79G/3.50G [00:12<00:03, 236MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  81%|████ | 2.82G/3.50G [00:12<00:02, 231MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  81%|████ | 2.85G/3.50G [00:12<00:02, 231MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  82%|████ | 2.88G/3.50G [00:13<00:02, 232MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  83%|████▏| 2.92G/3.50G [00:13<00:02, 231MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  84%|████▏| 2.95G/3.50G [00:13<00:02, 236MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  85%|████▎| 2.98G/3.50G [00:13<00:02, 242MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  86%|████▎| 3.01G/3.50G [00:13<00:01, 249MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  87%|████▎| 3.04G/3.50G [00:13<00:01, 247MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  88%|████▍| 3.07G/3.50G [00:13<00:01, 243MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  89%|████▍| 3.10G/3.50G [00:13<00:01, 243MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  90%|████▍| 3.14G/3.50G [00:14<00:01, 246MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  90%|████▌| 3.17G/3.50G [00:14<00:01, 244MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  91%|████▌| 3.20G/3.50G [00:14<00:01, 248MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  92%|████▌| 3.23G/3.50G [00:14<00:01, 248MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  93%|████▋| 3.26G/3.50G [00:14<00:00, 248MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  94%|████▋| 3.29G/3.50G [00:14<00:00, 253MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  95%|████▋| 3.32G/3.50G [00:14<00:00, 258MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  96%|████▊| 3.36G/3.50G [00:14<00:00, 261MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  97%|████▊| 3.39G/3.50G [00:15<00:00, 259MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  98%|████▉| 3.42G/3.50G [00:15<00:00, 255MB/s]\u001b[A\nmodel-00002-of-00002.safetensors:  99%|████▉| 3.45G/3.50G [00:15<00:00, 256MB/s]\u001b[A\nmodel-00002-of-00002.safetensors: 100%|█████| 3.50G/3.50G [00:15<00:00, 226MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 2/2 [00:54<00:00, 27.13s/it]\nDownloading shards: 100%|█████████████████████████| 2/2 [00:54<00:00, 27.15s/it]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [01:04<00:00, 32.48s/it]\ngeneration_config.json: 100%|██████████████████| 188/188 [00:00<00:00, 1.17MB/s]\nLoading checkpoint shards: 100%|██████████████████| 2/2 [01:05<00:00, 32.54s/it]\n[2024-05-18 17:37:05,765] [INFO] [axolotl.log_gpu_memory_usage:81] [PID:917] [RANK:0] GPU memory usage after model load: 3.710GB (+0.255GB cache, +0.458GB misc)\u001b[39m\n[2024-05-18 17:37:05,775] [INFO] [axolotl.load_model:775] [PID:917] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n[2024-05-18 17:37:05,781] [INFO] [axolotl.load_model:784] [PID:917] [RANK:0] converting modules to torch.float16 for flash attention\u001b[39m\n[2024-05-18 17:37:05,786] [INFO] [axolotl.load_lora:928] [PID:917] [RANK:0] found linear modules: ['v_proj', 'up_proj', 'q_proj', 'o_proj', 'k_proj', 'gate_proj', 'down_proj']\u001b[39m\n[2024-05-18 17:37:05,825] [INFO] [axolotl.log_gpu_memory_usage:81] [PID:918] [RANK:1] GPU memory usage after model load: 3.710GB (+0.255GB cache, +0.458GB misc)\u001b[39m\n[2024-05-18 17:37:05,833] [INFO] [axolotl.load_model:775] [PID:918] [RANK:1] converting PEFT model w/ prepare_model_for_kbit_training\u001b[39m\n[2024-05-18 17:37:05,839] [INFO] [axolotl.load_model:784] [PID:918] [RANK:1] converting modules to torch.float16 for flash attention\u001b[39m\n[2024-05-18 17:37:05,844] [INFO] [axolotl.load_lora:928] [PID:918] [RANK:1] found linear modules: ['v_proj', 'gate_proj', 'k_proj', 'q_proj', 'o_proj', 'up_proj', 'down_proj']\u001b[39m\ntrainable params: 79,953,920 || all params: 6,818,369,536 || trainable%: 1.172625208678628\n[2024-05-18 17:37:07,226] [INFO] [axolotl.log_gpu_memory_usage:81] [PID:917] [RANK:0] GPU memory usage after adapters: 3.859GB (+1.239GB cache, +0.460GB misc)\u001b[39m\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:832: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\n  warnings.warn(\"DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\")\n[2024-05-18 17:37:07,284] [INFO] [axolotl.log_gpu_memory_usage:81] [PID:918] [RANK:1] GPU memory usage after adapters: 3.859GB (+1.239GB cache, +0.460GB misc)\u001b[39m\n[2024-05-18 17:37:07,290] [INFO] [axolotl.train.log:63] [PID:917] [RANK:0] Pre-saving adapter config to ./outputs/qlora-out\u001b[39m\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/opt/conda/lib/python3.10/site-packages/accelerate/utils/dataclasses.py:832: UserWarning: DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\n  warnings.warn(\"DeepSpeed Zero3 Init flag is only applicable for ZeRO Stage 3. Setting it to False.\")\n[2024-05-18 17:37:07,332] [INFO] [axolotl.train.log:63] [PID:917] [RANK:0] Starting trainer...\u001b[39m\n[2024-05-18 17:37:07,725] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:917] [RANK:0] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:07,761] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:917] [RANK:0] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:07,770] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:918] [RANK:1] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:07,797] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:917] [RANK:0] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:07,807] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:918] [RANK:1] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:07,834] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:917] [RANK:0] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:07,844] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:918] [RANK:1] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:07,881] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:918] [RANK:1] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:08,310] [WARNING] [engine.py:1188:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n[2024-05-18 17:37:09,065] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:918] [RANK:1] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:09,116] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:918] [RANK:1] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n  0%|                                                   | 0/366 [00:00<?, ?it/s][2024-05-18 17:37:10,142] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:917] [RANK:0] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\n[2024-05-18 17:37:10,180] [INFO] [axolotl.utils.samplers.multipack._len_est:184] [PID:917] [RANK:0] packing_efficiency_estimate: 0.87 total_num_tokens per device: 2637743\u001b[39m\nYou're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.0}             \n  0%|                                         | 1/366 [00:19<2:00:33, 19.82s/it][2024-05-18 17:37:49,442] [INFO] [axolotl.callbacks.log_gpu_memory_usage:81] [PID:918] [RANK:1] GPU memory usage while training: 4.098GB (+4.121GB cache, +0.481GB misc)\u001b[39m\n[2024-05-18 17:37:49,442] [INFO] [axolotl.callbacks.log_gpu_memory_usage:81] [PID:917] [RANK:0] GPU memory usage while training: 4.098GB (+4.121GB cache, +0.481GB misc)\u001b[39m\n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.01}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.01}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.01}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.01}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.02}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.02}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.02}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.02}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.03}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.03}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.03}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.04}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.04}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.04}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.04}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.05}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.05}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.05}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.05}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.06}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.06}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.06}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.07}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.08}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.08}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.08}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.08}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.09}            \n{'loss': 0.0, 'grad_norm': 0.0, 'learning_rate': 0.0, 'epoch': 0.09}            \n  9%|███▌                                    | 33/366 [10:53<1:49:58, 19.81s/it]Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n        return _run_code(code, main_globals, None,return _run_code(code, main_globals, None,\n\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/kaggle/working/axolotl/src/axolotl/cli/train.py\", line 70, in <module>\n    exec(code, run_globals)\n  File \"/kaggle/working/axolotl/src/axolotl/cli/train.py\", line 70, in <module>\n    fire.Fire(do_cli)\n      File \"/opt/conda/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\nfire.Fire(do_cli)\n  File \"/opt/conda/lib/python3.10/site-packages/fire/core.py\", line 143, in Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"/opt/conda/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n  File \"/opt/conda/lib/python3.10/site-packages/fire/core.py\", line 477, in _Fire\n        component, remaining_args = _CallAndUpdateTrace(\ncomponent, remaining_args = _CallAndUpdateTrace(\n  File \"/opt/conda/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n  File \"/opt/conda/lib/python3.10/site-packages/fire/core.py\", line 693, in _CallAndUpdateTrace\n    component = fn(*varargs, **kwargs)\n  File \"/kaggle/working/axolotl/src/axolotl/cli/train.py\", line 38, in do_cli\n    component = fn(*varargs, **kwargs)\n  File \"/kaggle/working/axolotl/src/axolotl/cli/train.py\", line 38, in do_cli\n    return do_train(parsed_cfg, parsed_cli_args)\n  File \"/kaggle/working/axolotl/src/axolotl/cli/train.py\", line 66, in do_train\n    return train(cfg=cfg, cli_args=cli_args, dataset_meta=dataset_meta)\n  File \"/kaggle/working/axolotl/src/axolotl/train.py\", line 170, in train\n    return do_train(parsed_cfg, parsed_cli_args)\n  File \"/kaggle/working/axolotl/src/axolotl/cli/train.py\", line 66, in do_train\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1859, in train\n    return train(cfg=cfg, cli_args=cli_args, dataset_meta=dataset_meta)\n  File \"/kaggle/working/axolotl/src/axolotl/train.py\", line 170, in train\n    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 1859, in train\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2203, in _inner_training_loop\n    return inner_training_loop(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 2203, in _inner_training_loop\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3147, in training_step\n    tr_loss_step = self.training_step(model, inputs)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py\", line 3147, in training_step\n    self.accelerator.backward(loss)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2117, in backward\n    self.accelerator.backward(loss)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2117, in backward\n        self.deepspeed_engine_wrapped.backward(loss, **kwargs)\nself.deepspeed_engine_wrapped.backward(loss, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 175, in backward\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/deepspeed.py\", line 175, in backward\n    self.engine.step()\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2169, in step\n    self.engine.step()\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2169, in step\n    self._take_model_step(lr_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2075, in _take_model_step\n    self._take_model_step(lr_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/engine.py\", line 2075, in _take_model_step\n    self.optimizer.step()\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1828, in step\n    self.optimizer.step()\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 1828, in step\n    self._update_scale(self.overflow)\n      File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2066, in _update_scale\nself._update_scale(self.overflow)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py\", line 2066, in _update_scale\n    self.loss_scaler.update_scale(has_overflow)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 175, in update_scale\n    self.loss_scaler.update_scale(has_overflow)\n  File \"/opt/conda/lib/python3.10/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 175, in update_scale\n    raise Exception(\n    raise Exception(Exception\n: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.\nException: Current loss scale already at minimum - cannot decrease scale anymore. Exiting run.\n  9%|███▌                                    | 33/366 [11:13<1:53:20, 20.42s/it]\n[2024-05-18 17:48:30,517] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 917) of binary: /opt/conda/bin/python3.10\nTraceback (most recent call last):\n  File \"/opt/conda/bin/accelerate\", line 8, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py\", line 46, in main\n    args.func(args)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 1067, in launch_command\n    deepspeed_launcher(args)\n  File \"/opt/conda/lib/python3.10/site-packages/accelerate/commands/launch.py\", line 771, in deepspeed_launcher\n    distrib_run.run(args)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\n    elastic_launch(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\naxolotl.cli.train FAILED\n------------------------------------------------------------\nFailures:\n[1]:\n  time      : 2024-05-18_17:48:30\n  host      : eef26f91f727\n  rank      : 1 (local_rank: 1)\n  exitcode  : 1 (pid: 918)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2024-05-18_17:48:30\n  host      : eef26f91f727\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 917)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}